{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## State of art inspired mesh model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 260,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import string\n",
    "import tensorflow as tf\n",
    "import os\n",
    "import numpy as np\n",
    "from nltk.tokenize import word_tokenize\n",
    "from scipy.sparse import csr_matrix\n",
    "import pandas as pd\n",
    "from collections import Counter\n",
    "import re\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "import nltk\n",
    "import sklearn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 261,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(r'D:\\SEM-2\\NLP\\Project\\data\\hashtag_tweets.csv')\n",
    "TrainX = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 262,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>twitter_id</th>\n",
       "      <th>label</th>\n",
       "      <th>text</th>\n",
       "      <th>hashtag</th>\n",
       "      <th>name</th>\n",
       "      <th>geo</th>\n",
       "      <th>coordinates</th>\n",
       "      <th>screen_name</th>\n",
       "      <th>description</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>572342978255048705</td>\n",
       "      <td>racism</td>\n",
       "      <td>So Drasko just said he was impressed the girls...</td>\n",
       "      <td>['mkr']</td>\n",
       "      <td>thefoxbandit</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>thefoxbandit</td>\n",
       "      <td>pro gamer,drawer, full time nerd and emo</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>572341498827522049</td>\n",
       "      <td>racism</td>\n",
       "      <td>Drasko they didn't cook half a bird you idiot ...</td>\n",
       "      <td>['mkr']</td>\n",
       "      <td>patricia hilder</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>trish2295</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "           twitter_id   label  \\\n",
       "0  572342978255048705  racism   \n",
       "1  572341498827522049  racism   \n",
       "\n",
       "                                                text  hashtag  \\\n",
       "0  So Drasko just said he was impressed the girls...  ['mkr']   \n",
       "1  Drasko they didn't cook half a bird you idiot ...  ['mkr']   \n",
       "\n",
       "              name  geo  coordinates   screen_name  \\\n",
       "0     thefoxbandit  NaN          NaN  thefoxbandit   \n",
       "1  patricia hilder  NaN          NaN     trish2295   \n",
       "\n",
       "                                description  \n",
       "0  pro gamer,drawer, full time nerd and emo  \n",
       "1                                       NaN  "
      ]
     },
     "execution_count": 262,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df[:2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 263,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['text'] = df['text'].replace(np.nan, 'no_text')\n",
    "df['description'] = df['description'].replace(np.nan, 'no_desc')\n",
    "df['screen_name'] = df['screen_name'].replace(np.nan, 'no_name')\n",
    "df['hashtag'] = df['hashtag'].replace(np.nan, 'no_hash')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 264,
   "metadata": {},
   "outputs": [],
   "source": [
    "TrainX1 = df['text']\n",
    "TrainX2 = df['screen_name']\n",
    "TrainX3 = df['description']\n",
    "TrainX4 = df['hashtag']\n",
    "TrainY = df['label']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 265,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Counter({'racism': 12,\n",
       "         'sexism': 2708,\n",
       "         \"[{'text': 'feminists', 'indices': [49, 59]}, {'text': 'yesallmen', 'indices': [60, 70]}]\": 1,\n",
       "         'none': 7626})"
      ]
     },
     "execution_count": 265,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from collections import Counter\n",
    "Counter(TrainY)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 266,
   "metadata": {},
   "outputs": [],
   "source": [
    "TrainY_fin = np.ones(len(df))\n",
    "for i in range(len(df)):\n",
    "    if TrainY[i]=='none':\n",
    "        TrainY_fin[i] = 0\n",
    "TrainY = TrainY_fin"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 267,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess(sent):\n",
    "    sent = sent.lower()\n",
    "    language = string.ascii_lowercase + string.digits + string.punctuation + ' '\n",
    "    #to filter out unicode or foreign characters\n",
    "    sent = \"\".join([char for char in sent if char in language])\n",
    "    #remove all HTML tags\n",
    "    sent = sent.replace(\"<.*?>\", \" \")\n",
    "    #replacing multiple spaces with single space\n",
    "    sent = re.sub(r\"\\.+(\\s?\\.+)*\", \".\", sent)\n",
    "    #introducing space after punctuation if not already there\n",
    "    sent = re.sub(r\"([,\\.\\\"'\\?])(?!\\s)\", r\"\\1 \", sent) \n",
    "    #introducing space before punctuation if not already there\n",
    "    sent = re.sub(r\"(?<!\\s)([,\\.\\\"'\\?])\", r\" \\1\", sent) \n",
    "    #replacing words ending in \"n't\" with \"word not\"\n",
    "    sent = re.sub(r\"([a-zA-z])(n't)\", r\"\\1 not\", sent) \n",
    "    #replacing multiple spaces with single space\n",
    "    sent = re.sub(r\"\\s+\", r\" \", sent)\n",
    "    return sent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 268,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Counter({1.0: 2721, 0.0: 7626})"
      ]
     },
     "execution_count": 268,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Counter(TrainY_fin)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Preperation of the first type of input - Tweet text (Text data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 269,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10347 10347\n"
     ]
    }
   ],
   "source": [
    "print(len(TrainX1),len(TrainY))\n",
    "X = np.array(TrainX1)\n",
    "Y = np.array(TrainY)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 270,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = sklearn.model_selection.train_test_split(X, Y, test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 271,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "8277"
      ]
     },
     "execution_count": 271,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "size_of_input = X_train.shape[0]\n",
    "size_of_input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 272,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8277 2070\n"
     ]
    }
   ],
   "source": [
    "print(len(X_train),len(X_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 273,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab = set(X_train[0])\n",
    "for i in range(X_train.shape[0]):\n",
    "    X_train[i] = X_train[i][:75]\n",
    "    current_dict =  set(X_train[i].split())\n",
    "    vocab = vocab.union(current_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 274,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1194\n"
     ]
    }
   ],
   "source": [
    "vocab_size1 = len(vocab)\n",
    "embedding_dim = 10\n",
    "max_length = 75\n",
    "trunc_type='post'\n",
    "oov_tok = \"<OOV>\"\n",
    "print(vocab_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 275,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tokenization\n",
    "tokenizer = tf.keras.preprocessing.text.Tokenizer(num_words = vocab_size, oov_token=oov_tok)\n",
    "tokenizer.fit_on_texts(X_train) \n",
    "\n",
    "word_index = tokenizer.word_index\n",
    "sequences = tokenizer.texts_to_sequences(X_train)\n",
    "testing_sequences = tokenizer.texts_to_sequences(X_test)\n",
    "\n",
    "# Padding\n",
    "padded = tf.keras.preprocessing.sequence.pad_sequences(sequences, maxlen=max_length, truncating=trunc_type)\n",
    "testing_padded = tf.keras.preprocessing.sequence.pad_sequences(testing_sequences, maxlen=max_length)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 276,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train padded shape: (8277, 75)\n",
      "Test padded shape: (2070, 75)\n"
     ]
    }
   ],
   "source": [
    "print(f'Train padded shape: {padded.shape}')\n",
    "print(f'Test padded shape: {testing_padded.shape}')\n",
    "\n",
    "padded = tf.convert_to_tensor(padded)\n",
    "testing_padded = tf.convert_to_tensor(testing_padded)\n",
    "\n",
    "\n",
    "X_train=np.asarray(padded).astype(np.int)\n",
    "y_train=np.asarray(y_train).astype(np.int)\n",
    "X_test=np.asarray(testing_padded).astype(np.int)\n",
    "y_test=np.asarray(y_test).astype(np.int)\n",
    "embedding_Layer = tf.keras.layers.Embedding(vocab_size, 8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 277,
   "metadata": {},
   "outputs": [],
   "source": [
    "inputA1 =  X_train\n",
    "inputA_test =  X_test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Preperation of second type of input -  Screen name (Meta data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 278,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10347 10347\n"
     ]
    }
   ],
   "source": [
    "print(len(TrainX3),len(TrainY))\n",
    "X = np.array(TrainX3)\n",
    "Y = np.array(TrainY)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 279,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(10347,) (10347,)\n"
     ]
    }
   ],
   "source": [
    "print(X.shape,Y.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 280,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = sklearn.model_selection.train_test_split(X, Y, test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 281,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8277 2070\n"
     ]
    }
   ],
   "source": [
    "print(len(X_train),len(X_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 282,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab = set(X_train[0])\n",
    "for i in range(X_train.shape[0]):\n",
    "    X_train[i] = X_train[i][:75]\n",
    "    current_dict =  set(X_train[i].split())\n",
    "    vocab = vocab.union(current_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 283,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1194\n"
     ]
    }
   ],
   "source": [
    "vocab_size2 = len(vocab)\n",
    "embedding_dim = 10\n",
    "max_length = 75\n",
    "trunc_type='post'\n",
    "oov_tok = \"<OOV>\"\n",
    "print(vocab_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 284,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tokenization\n",
    "tokenizer = tf.keras.preprocessing.text.Tokenizer(num_words = vocab_size, oov_token=oov_tok)\n",
    "tokenizer.fit_on_texts(X_train) \n",
    "\n",
    "word_index = tokenizer.word_index\n",
    "sequences = tokenizer.texts_to_sequences(X_train)\n",
    "testing_sequences = tokenizer.texts_to_sequences(X_test)\n",
    "\n",
    "# Padding\n",
    "padded = tf.keras.preprocessing.sequence.pad_sequences(sequences, maxlen=max_length, truncating=trunc_type)\n",
    "testing_padded = tf.keras.preprocessing.sequence.pad_sequences(testing_sequences, maxlen=max_length)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 285,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train padded shape: (8277, 75)\n",
      "Test padded shape: (2070, 75)\n"
     ]
    }
   ],
   "source": [
    "print(f'Train padded shape: {padded.shape}')\n",
    "print(f'Test padded shape: {testing_padded.shape}')\n",
    "\n",
    "padded = tf.convert_to_tensor(padded)\n",
    "testing_padded = tf.convert_to_tensor(testing_padded)\n",
    "\n",
    "\n",
    "X_train=np.asarray(padded).astype(np.int)\n",
    "y_train=np.asarray(y_train).astype(np.int)\n",
    "X_test=np.asarray(testing_padded).astype(np.int)\n",
    "y_test=np.asarray(y_test).astype(np.int)\n",
    "embedding_Layer = tf.keras.layers.Embedding(vocab_size, 8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 286,
   "metadata": {},
   "outputs": [],
   "source": [
    "inputB1 =  X_train\n",
    "inputB_test =  X_test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Preperation of third type of input: User-bio (user-data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 287,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10347 10347\n"
     ]
    }
   ],
   "source": [
    "print(len(TrainX2),len(TrainY))\n",
    "X = np.array(TrainX2)\n",
    "Y = np.array(TrainY)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 288,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = sklearn.model_selection.train_test_split(X, Y, test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 289,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(8277,) (8277,)\n"
     ]
    }
   ],
   "source": [
    "print(X_train.shape,y_train.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 290,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab = set(X_train[0])\n",
    "for i in range(X_train.shape[0]):\n",
    "    X_train[i] = X_train[i][:75]\n",
    "    current_dict =  set(X_train[i].split())\n",
    "    vocab = vocab.union(current_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 291,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1194\n"
     ]
    }
   ],
   "source": [
    "vocab_size3 = len(vocab)\n",
    "embedding_dim = 10\n",
    "max_length = 75\n",
    "trunc_type='post'\n",
    "oov_tok = \"<OOV>\"\n",
    "print(vocab_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 292,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tokenization\n",
    "tokenizer = tf.keras.preprocessing.text.Tokenizer(num_words = vocab_size, oov_token=oov_tok)\n",
    "tokenizer.fit_on_texts(X_train) \n",
    "\n",
    "word_index = tokenizer.word_index\n",
    "sequences = tokenizer.texts_to_sequences(X_train)\n",
    "testing_sequences = tokenizer.texts_to_sequences(X_test)\n",
    "\n",
    "# Padding\n",
    "padded = tf.keras.preprocessing.sequence.pad_sequences(sequences, maxlen=max_length, truncating=trunc_type)\n",
    "testing_padded = tf.keras.preprocessing.sequence.pad_sequences(testing_sequences, maxlen=max_length)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 293,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train padded shape: (8277, 75)\n",
      "Test padded shape: (2070, 75)\n"
     ]
    }
   ],
   "source": [
    "print(f'Train padded shape: {padded.shape}')\n",
    "print(f'Test padded shape: {testing_padded.shape}')\n",
    "\n",
    "padded = tf.convert_to_tensor(padded)\n",
    "testing_padded = tf.convert_to_tensor(testing_padded)\n",
    "\n",
    "\n",
    "X_train=np.asarray(padded).astype(np.int)\n",
    "y_train=np.asarray(y_train).astype(np.int)\n",
    "X_test=np.asarray(testing_padded).astype(np.int)\n",
    "y_test=np.asarray(y_test).astype(np.int)\n",
    "embedding_Layer = tf.keras.layers.Embedding(vocab_size, 8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 294,
   "metadata": {},
   "outputs": [],
   "source": [
    "inputC1 =  X_train\n",
    "inputC_test =  X_test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Preperation of fourth type of input (hashtag: text data but no sequential element present)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 295,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10347 10347\n"
     ]
    }
   ],
   "source": [
    "print(len(TrainX4),len(TrainY))\n",
    "X = np.array(TrainX4)\n",
    "Y = np.array(TrainY)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 296,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = sklearn.model_selection.train_test_split(X, Y, test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 297,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(8277,) (8277,)\n"
     ]
    }
   ],
   "source": [
    "print(X_train.shape,y_train.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 298,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab = set(X_train[0])\n",
    "for i in range(X_train.shape[0]):\n",
    "    X_train[i] = X_train[i][:75]\n",
    "    current_dict =  set(X_train[i].split())\n",
    "    vocab = vocab.union(current_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 299,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1194\n"
     ]
    }
   ],
   "source": [
    "vocab_size4 = len(vocab)\n",
    "embedding_dim = 10\n",
    "max_length = 75\n",
    "trunc_type='post'\n",
    "oov_tok = \"<OOV>\"\n",
    "print(vocab_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 300,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tokenization\n",
    "tokenizer = tf.keras.preprocessing.text.Tokenizer(num_words = vocab_size, oov_token=oov_tok)\n",
    "tokenizer.fit_on_texts(X_train) \n",
    "\n",
    "word_index = tokenizer.word_index\n",
    "sequences = tokenizer.texts_to_sequences(X_train)\n",
    "testing_sequences = tokenizer.texts_to_sequences(X_test)\n",
    "\n",
    "# Padding\n",
    "padded = tf.keras.preprocessing.sequence.pad_sequences(sequences, maxlen=max_length, truncating=trunc_type)\n",
    "testing_padded = tf.keras.preprocessing.sequence.pad_sequences(testing_sequences, maxlen=max_length)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 301,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train padded shape: (8277, 75)\n",
      "Test padded shape: (2070, 75)\n"
     ]
    }
   ],
   "source": [
    "print(f'Train padded shape: {padded.shape}')\n",
    "print(f'Test padded shape: {testing_padded.shape}')\n",
    "\n",
    "padded = tf.convert_to_tensor(padded)\n",
    "testing_padded = tf.convert_to_tensor(testing_padded)\n",
    "\n",
    "\n",
    "X_train=np.asarray(padded).astype(np.int)\n",
    "y_train=np.asarray(y_train).astype(np.int)\n",
    "X_test=np.asarray(testing_padded).astype(np.int)\n",
    "y_test=np.asarray(y_test).astype(np.int)\n",
    "embedding_Layer = tf.keras.layers.Embedding(vocab_size, 8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 302,
   "metadata": {},
   "outputs": [],
   "source": [
    "inputD1 =  X_train\n",
    "inputD_test =  X_test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Model definition | The output from each of the models is concatenated and fed into another dense layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 303,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.layers import Input, Embedding, LSTM, Dense \n",
    "embedding_dim = 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 304,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_89\"\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_90 (InputLayer)           [(None, 75)]         0                                            \n",
      "__________________________________________________________________________________________________\n",
      "input_91 (InputLayer)           [(None, 75)]         0                                            \n",
      "__________________________________________________________________________________________________\n",
      "embedding_83 (Embedding)        (None, 75, 10)       202180      input_90[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "embedding_84 (Embedding)        (None, 75, 10)       49010       input_91[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "bidirectional_41 (Bidirectional (None, 20)           1680        embedding_83[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "bidirectional_42 (Bidirectional (None, 20)           1680        embedding_84[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "dense_353 (Dense)               (None, 4)            84          bidirectional_41[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "dense_358 (Dense)               (None, 4)            84          bidirectional_42[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "input_92 (InputLayer)           [(None, 75)]         0                                            \n",
      "__________________________________________________________________________________________________\n",
      "input_93 (InputLayer)           [(None, 75)]         0                                            \n",
      "__________________________________________________________________________________________________\n",
      "dense_354 (Dense)               (None, 8)            40          dense_353[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "dense_359 (Dense)               (None, 8)            40          dense_358[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "dense_363 (Dense)               (None, 64)           4864        input_92[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "dense_367 (Dense)               (None, 64)           4864        input_93[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "dense_355 (Dense)               (None, 16)           144         dense_354[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "dense_360 (Dense)               (None, 16)           144         dense_359[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "dense_364 (Dense)               (None, 32)           2080        dense_363[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "dense_368 (Dense)               (None, 32)           2080        dense_367[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "dense_356 (Dense)               (None, 10)           170         dense_355[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "dense_361 (Dense)               (None, 10)           170         dense_360[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "dense_365 (Dense)               (None, 10)           330         dense_364[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "dense_369 (Dense)               (None, 10)           330         dense_368[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "dense_357 (Dense)               (None, 1)            11          dense_356[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "dense_362 (Dense)               (None, 1)            11          dense_361[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "dense_366 (Dense)               (None, 1)            11          dense_365[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "dense_370 (Dense)               (None, 1)            11          dense_369[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "tf.concat_17 (TFOpLambda)       (None, 4)            0           dense_357[0][0]                  \n",
      "                                                                 dense_362[0][0]                  \n",
      "                                                                 dense_366[0][0]                  \n",
      "                                                                 dense_370[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "dense_371 (Dense)               (None, 64)           320         tf.concat_17[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "dense_372 (Dense)               (None, 1)            65          dense_371[0][0]                  \n",
      "==================================================================================================\n",
      "Total params: 270,403\n",
      "Trainable params: 270,403\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "inputA = tf.keras.Input(shape=(75))\n",
    "inputB = tf.keras.Input(shape=(75))\n",
    "inputC = tf.keras.Input(shape=(75))\n",
    "inputD = tf.keras.Input(shape=(75))\n",
    "\n",
    "# the first branch operates on the first input\n",
    "x1 = tf.keras.layers.Embedding(vocab_size1, embedding_dim)(inputA)\n",
    "x1 = tf.keras.layers.Bidirectional(tf.keras.layers.LSTM(embedding_dim))(x1)\n",
    "x1 = tf.keras.layers.Dense(4, activation=\"relu\")(x1)\n",
    "x1 = tf.keras.layers.Dense(8, activation=\"relu\")(x1)\n",
    "x1 = tf.keras.layers.Dense(16, activation=\"relu\")(x1)\n",
    "x1 = tf.keras.layers.Dense(embedding_dim, activation=\"relu\")(x1)\n",
    "x1 = tf.keras.layers.Dense(1, activation=\"sigmoid\")(x1)\n",
    "x1 = tf.keras.Model(inputs=inputA, outputs=x1)\n",
    "\n",
    "# the second branch opreates on the second input\n",
    "x2 = tf.keras.layers.Embedding(vocab_size2, embedding_dim)(inputB)\n",
    "x2 = tf.keras.layers.Bidirectional(tf.keras.layers.LSTM(embedding_dim))(x2)\n",
    "x2 = tf.keras.layers.Dense(4, activation=\"relu\")(x2)\n",
    "x2 = tf.keras.layers.Dense(8, activation=\"relu\")(x2)\n",
    "x2 = tf.keras.layers.Dense(16, activation=\"relu\")(x2)\n",
    "x2 = tf.keras.layers.Dense(embedding_dim, activation=\"relu\")(x2)\n",
    "x2 = tf.keras.layers.Dense(1, activation=\"sigmoid\")(x2)\n",
    "x2 = tf.keras.Model(inputs=inputB, outputs=x2)\n",
    "\n",
    "\n",
    "# the third branch opreates on the second input\n",
    "x3 = tf.keras.layers.Dense(64, activation=\"relu\")(inputC)\n",
    "x3 = tf.keras.layers.Dense(32, activation=\"relu\")(x3)\n",
    "x3 = tf.keras.layers.Dense(embedding_dim, activation=\"relu\")(x3)\n",
    "x3 = tf.keras.layers.Dense(1, activation=\"sigmoid\")(x3)\n",
    "x3 = tf.keras.Model(inputs=inputC, outputs=x3)\n",
    "\n",
    "\n",
    "# the fourth branch opreates on the second input\n",
    "x4 = tf.keras.layers.Dense(64, activation=\"relu\")(inputD)\n",
    "x4 = tf.keras.layers.Dense(32, activation=\"relu\")(x4)\n",
    "x4 = tf.keras.layers.Dense(embedding_dim, activation=\"relu\")(x4)\n",
    "x4 = tf.keras.layers.Dense(1, activation=\"sigmoid\")(x4)\n",
    "x4 = tf.keras.Model(inputs=inputD, outputs=x4)\n",
    "\n",
    "\n",
    "# combine the output of the two branches\n",
    "combined = tf.concat([x1.output, x2.output, x3.output, x4.output],axis=1\n",
    ")\n",
    "\n",
    "z =  tf.keras.layers.Dense(64, activation='relu')(combined)\n",
    "z =  tf.keras.layers.Dense(1, activation='relu')(z)\n",
    "\n",
    "model = tf.keras.Model(inputs=[x1.input, x2.input, x3.input, x4.input], outputs=z)\n",
    "model.summary()\n",
    "model.compile(loss='binary_crossentropy',optimizer='Adam',metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Training the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 306,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50\n",
      "259/259 [==============================] - 4s 17ms/step - loss: 0.1897 - accuracy: 0.9413\n",
      "Epoch 2/50\n",
      "259/259 [==============================] - 4s 17ms/step - loss: 0.3534 - accuracy: 0.8823\n",
      "Epoch 3/50\n",
      "259/259 [==============================] - 4s 17ms/step - loss: 0.2751 - accuracy: 0.9083\n",
      "Epoch 4/50\n",
      "259/259 [==============================] - 4s 17ms/step - loss: 0.2208 - accuracy: 0.9307\n",
      "Epoch 5/50\n",
      "259/259 [==============================] - 4s 17ms/step - loss: 0.2076 - accuracy: 0.9375\n",
      "Epoch 6/50\n",
      "259/259 [==============================] - 4s 17ms/step - loss: 0.1945 - accuracy: 0.9444\n",
      "Epoch 7/50\n",
      "259/259 [==============================] - 4s 17ms/step - loss: 0.1848 - accuracy: 0.9467\n",
      "Epoch 8/50\n",
      "259/259 [==============================] - 4s 17ms/step - loss: 0.2070 - accuracy: 0.9385\n",
      "Epoch 9/50\n",
      "259/259 [==============================] - 4s 17ms/step - loss: 0.3161 - accuracy: 0.9319\n",
      "Epoch 10/50\n",
      "259/259 [==============================] - 4s 17ms/step - loss: 0.2896 - accuracy: 0.9171\n",
      "Epoch 11/50\n",
      "259/259 [==============================] - 4s 17ms/step - loss: 0.1799 - accuracy: 0.9410\n",
      "Epoch 12/50\n",
      "259/259 [==============================] - 4s 17ms/step - loss: 0.1752 - accuracy: 0.9464\n",
      "Epoch 13/50\n",
      "259/259 [==============================] - 4s 17ms/step - loss: 0.2138 - accuracy: 0.9459 0s - loss: 0.2189 - ac\n",
      "Epoch 14/50\n",
      "259/259 [==============================] - 4s 17ms/step - loss: 0.1734 - accuracy: 0.9484\n",
      "Epoch 15/50\n",
      "259/259 [==============================] - 4s 17ms/step - loss: 0.1462 - accuracy: 0.9551\n",
      "Epoch 16/50\n",
      "259/259 [==============================] - 4s 17ms/step - loss: 0.1503 - accuracy: 0.9574\n",
      "Epoch 17/50\n",
      "259/259 [==============================] - 4s 17ms/step - loss: 0.1536 - accuracy: 0.9590\n",
      "Epoch 18/50\n",
      "259/259 [==============================] - 5s 18ms/step - loss: 0.1586 - accuracy: 0.9611\n",
      "Epoch 19/50\n",
      "259/259 [==============================] - 4s 17ms/step - loss: 0.1557 - accuracy: 0.9606\n",
      "Epoch 20/50\n",
      "259/259 [==============================] - 4s 17ms/step - loss: 0.1685 - accuracy: 0.9605\n",
      "Epoch 21/50\n",
      "259/259 [==============================] - 5s 18ms/step - loss: 0.2112 - accuracy: 0.9590\n",
      "Epoch 22/50\n",
      "259/259 [==============================] - 4s 17ms/step - loss: 0.1802 - accuracy: 0.9507\n",
      "Epoch 23/50\n",
      "259/259 [==============================] - 4s 17ms/step - loss: 0.2988 - accuracy: 0.9140\n",
      "Epoch 24/50\n",
      "259/259 [==============================] - 4s 17ms/step - loss: 0.1777 - accuracy: 0.9632\n",
      "Epoch 25/50\n",
      "259/259 [==============================] - 4s 17ms/step - loss: 0.1611 - accuracy: 0.9646\n",
      "Epoch 26/50\n",
      "148/259 [================>.............] - ETA: 1s - loss: 0.1806 - accuracy: 0.9688"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-306-493be58c1e05>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mhistory\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0minputA1\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minputB1\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0minputC1\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0minputD1\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0my_train\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mepochs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m50\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32mC:\\anaconda\\lib\\site-packages\\keras\\engine\\training.py\u001b[0m in \u001b[0;36mfit\u001b[1;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_batch_size, validation_freq, max_queue_size, workers, use_multiprocessing)\u001b[0m\n\u001b[0;32m   1182\u001b[0m                 _r=1):\n\u001b[0;32m   1183\u001b[0m               \u001b[0mcallbacks\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mon_train_batch_begin\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mstep\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1184\u001b[1;33m               \u001b[0mtmp_logs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtrain_function\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0miterator\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1185\u001b[0m               \u001b[1;32mif\u001b[0m \u001b[0mdata_handler\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshould_sync\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1186\u001b[0m                 \u001b[0mcontext\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0masync_wait\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\anaconda\\lib\\site-packages\\tensorflow\\python\\eager\\def_function.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, *args, **kwds)\u001b[0m\n\u001b[0;32m    883\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    884\u001b[0m       \u001b[1;32mwith\u001b[0m \u001b[0mOptionalXlaContext\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_jit_compile\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 885\u001b[1;33m         \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    886\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    887\u001b[0m       \u001b[0mnew_tracing_count\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mexperimental_get_tracing_count\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\anaconda\\lib\\site-packages\\tensorflow\\python\\eager\\def_function.py\u001b[0m in \u001b[0;36m_call\u001b[1;34m(self, *args, **kwds)\u001b[0m\n\u001b[0;32m    915\u001b[0m       \u001b[1;31m# In this case we have created variables on the first call, so we run the\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    916\u001b[0m       \u001b[1;31m# defunned version which is guaranteed to never create variables.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 917\u001b[1;33m       \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_stateless_fn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[1;33m)\u001b[0m  \u001b[1;31m# pylint: disable=not-callable\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    918\u001b[0m     \u001b[1;32melif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_stateful_fn\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    919\u001b[0m       \u001b[1;31m# Release the lock early so that multiple threads can perform the call\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\anaconda\\lib\\site-packages\\tensorflow\\python\\eager\\function.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   3037\u001b[0m       (graph_function,\n\u001b[0;32m   3038\u001b[0m        filtered_flat_args) = self._maybe_define_function(args, kwargs)\n\u001b[1;32m-> 3039\u001b[1;33m     return graph_function._call_flat(\n\u001b[0m\u001b[0;32m   3040\u001b[0m         filtered_flat_args, captured_inputs=graph_function.captured_inputs)  # pylint: disable=protected-access\n\u001b[0;32m   3041\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\anaconda\\lib\\site-packages\\tensorflow\\python\\eager\\function.py\u001b[0m in \u001b[0;36m_call_flat\u001b[1;34m(self, args, captured_inputs, cancellation_manager)\u001b[0m\n\u001b[0;32m   1961\u001b[0m         and executing_eagerly):\n\u001b[0;32m   1962\u001b[0m       \u001b[1;31m# No tape is watching; skip to running the function.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1963\u001b[1;33m       return self._build_call_outputs(self._inference_function.call(\n\u001b[0m\u001b[0;32m   1964\u001b[0m           ctx, args, cancellation_manager=cancellation_manager))\n\u001b[0;32m   1965\u001b[0m     forward_backward = self._select_forward_and_backward_functions(\n",
      "\u001b[1;32mC:\\anaconda\\lib\\site-packages\\tensorflow\\python\\eager\\function.py\u001b[0m in \u001b[0;36mcall\u001b[1;34m(self, ctx, args, cancellation_manager)\u001b[0m\n\u001b[0;32m    589\u001b[0m       \u001b[1;32mwith\u001b[0m \u001b[0m_InterpolateFunctionError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    590\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mcancellation_manager\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 591\u001b[1;33m           outputs = execute.execute(\n\u001b[0m\u001b[0;32m    592\u001b[0m               \u001b[0mstr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msignature\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mname\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    593\u001b[0m               \u001b[0mnum_outputs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_num_outputs\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\anaconda\\lib\\site-packages\\tensorflow\\python\\eager\\execute.py\u001b[0m in \u001b[0;36mquick_execute\u001b[1;34m(op_name, num_outputs, inputs, attrs, ctx, name)\u001b[0m\n\u001b[0;32m     57\u001b[0m   \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     58\u001b[0m     \u001b[0mctx\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mensure_initialized\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 59\u001b[1;33m     tensors = pywrap_tfe.TFE_Py_Execute(ctx._handle, device_name, op_name,\n\u001b[0m\u001b[0;32m     60\u001b[0m                                         inputs, attrs, num_outputs)\n\u001b[0;32m     61\u001b[0m   \u001b[1;32mexcept\u001b[0m \u001b[0mcore\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_NotOkStatusException\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "history = model.fit([inputA1, inputB1,inputC1,inputD1],y_train,epochs=50,)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Testing the performance on unseen test data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 307,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "65/65 [==============================] - 1s 3ms/step - loss: 0.8658 - accuracy: 0.8754\n"
     ]
    }
   ],
   "source": [
    "metrix = model.evaluate([inputA_test,inputB_test,inputC_test,inputD_test],y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A test-accuracy of 87 percent is obtained!"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
